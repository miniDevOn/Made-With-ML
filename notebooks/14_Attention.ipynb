{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "14_Attention",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgZTKjbZ6kfz"
      },
      "source": [
        "<div align=\"center\">\n",
        "<h1><img width=\"30\" src=\"https://madewithml.com/static/images/rounded_logo.png\">&nbsp;<a href=\"https://madewithml.com/\">Made With ML</a></h1>\n",
        "Applied ML Â· MLOps Â· Production\n",
        "<br>\n",
        "Join 30K+ developers in learning how to responsibly <a href=\"https://madewithml.com/about/\">deliver value</a> with ML.\n",
        "    <br>\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "\n",
        "<div align=\"center\">\n",
        "    <a target=\"_blank\" href=\"https://newsletter.madewithml.com\"><img src=\"https://img.shields.io/badge/Subscribe-30K-brightgreen\"></a>&nbsp;\n",
        "    <a target=\"_blank\" href=\"https://github.com/GokuMohandas/MadeWithML\"><img src=\"https://img.shields.io/github/stars/GokuMohandas/MadeWithML.svg?style=social&label=Star\"></a>&nbsp;\n",
        "    <a target=\"_blank\" href=\"https://www.linkedin.com/in/goku\"><img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\"></a>&nbsp;\n",
        "    <a target=\"_blank\" href=\"https://twitter.com/GokuMohandas\"><img src=\"https://img.shields.io/twitter/follow/GokuMohandas.svg?label=Follow&style=social\"></a>\n",
        "    <br>\n",
        "    ðŸ”¥&nbsp; Among the <a href=\"https://github.com/topics/deep-learning\" target=\"_blank\">top ML</a> repositories on GitHub\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTdCMVl9YAXw"
      },
      "source": [
        "# Attention\n",
        "\n",
        "In this lesson we will learn how to incorporate attention mechanisms to create more context-aware representations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuabAj4PYj57"
      },
      "source": [
        "<div align=\"left\">\n",
        "<a target=\"_blank\" href=\"https://madewithml.com/courses/foundations/attention/\"><img src=\"https://img.shields.io/badge/ðŸ“– Read-blog post-9cf\"></a>&nbsp;\n",
        "<a href=\"https://github.com/GokuMohandas/MadeWithML/blob/main/notebooks/14_Attention.ipynb\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n",
        "<a href=\"https://colab.research.google.com/github/GokuMohandas/MadeWithML/blob/main/notebooks/14_Attention.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5JJY0tl8StF"
      },
      "source": [
        "# Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDYX2d_eySKj"
      },
      "source": [
        "In the <a target=\"_blank\" href=\"https://madewithml.com/courses/foundations/recurrent-neural-networks/\">RNN lesson</a>, we were constrained to using the representation at the very end but what if we could give contextual weight to each encoded input ($h_i$) when making our prediction? This is also preferred because it can help mitigate the vanishing gradient issue which stems from processing very long sequences.\n",
        "\n",
        "Below is attention applied to the outputs from an RNN. In theory, the outputs can come from anywhere where we want to learn how to weight amongst them but since we're working with the context of an RNN from the previous lesson , we'll continue with that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34Yf1pdURtLn"
      },
      "source": [
        "<div align=\"left\">\n",
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/images/foundations/attention/attention.png\" width=\"500\">\n",
        "</div>\n",
        "\n",
        "$ \\alpha = softmax(W_{attn}h) $\n",
        "\n",
        "$c_t = \\sum_{i=1}^{n} \\alpha_{t,i}h_i $\n",
        "\n",
        "\n",
        "*where*:\n",
        "* $ h $ = RNN outputs (or any group of outputs you want to attend to) $\\in \\mathbb{R}^{NXMXH}$ ($N$ is the batch size, $M$ is the max sequence length in the batch, $H$ is the hidden dim)\n",
        "* $ \\alpha_{t,i} $ = alignment function for output $ y_t $ using input $ h_i $ (we also concatenate other useful representations with $h_i$ here). In our case, this would be the attention value to attribute to each input $h_i$. \n",
        "* $W_{attn}$ = attention weights to learn $\\in \\mathbb{R}^{HX1}$. We can also apply activations functions, transformations, etc. here\n",
        "* $c_t$ = context vector that accounts for the different inputs with attention. We can pass this context vector to downstream processes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqxyljU18hvt"
      },
      "source": [
        "* **Objective:**  At it's core, attention is about learning how to weigh a group of encoded representations to produce a context-aware representation to use for downstream tasks. This is done by learning a set of attention weights and then using softmax to create attention values that sum to 1.\n",
        "* **Advantages:** \n",
        "    * Learn how to account for the appropriate encoded representations regardless of position.\n",
        "* **Disadvantages:** \n",
        "    * Another compute step that involves learning weights.\n",
        "* **Miscellaneous:** \n",
        "    * Several state-of-the-art approaches extend on basic attention to deliver highly context-aware representations (ex. self-attention). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V88LZeTqDePU"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlRv80v5DfdP"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoNhL0wTDgLW"
      },
      "source": [
        "SEED = 1234"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwVLi0UPDgOd"
      },
      "source": [
        "def set_seeds(seed=1234):\n",
        "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # multi-GPU# Set seeds for reproducibility\n",
        "set_seeds(seed=SEED)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HK5Onfz9DiPO"
      },
      "source": [
        "# Set seeds for reproducibility\n",
        "set_seeds(seed=SEED)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4T1rZDhsDiRt",
        "outputId": "372e1317-1ed0-46c5-c136-553f1064f31f"
      },
      "source": [
        "# Set device\n",
        "cuda = True\n",
        "device = torch.device('cuda' if (\n",
        "    torch.cuda.is_available() and cuda) else 'cpu')\n",
        "torch.set_default_tensor_type('torch.FloatTensor')\n",
        "if device.type == 'cuda':\n",
        "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "print (device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c69z9wpJ56nE"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V_nEp5G58M0"
      },
      "source": [
        "We will download the [AG News dataset](http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html), which consists of 120K text samples from 4 unique classes (`Business`, `Sci/Tech`, `Sports`, `World`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3qKSoEe57na"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import urllib"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqaHCtToXWAb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f87fb125-c1d6-4250-c863-16bf5397e73b"
      },
      "source": [
        "# Load data\n",
        "url = \"https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/datasets/news.csv\"\n",
        "df = pd.read_csv(url, header=0) # load\n",
        "df = df.sample(frac=1).reset_index(drop=True) # shuffle\n",
        "df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sharon Accepts Plan to Reduce Gaza Army Operat...</td>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Internet Key Battleground in Wildlife Crime Fight</td>\n",
              "      <td>Sci/Tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>July Durable Good Orders Rise 1.7 Percent</td>\n",
              "      <td>Business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Growing Signs of a Slowing on Wall Street</td>\n",
              "      <td>Business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The New Faces of Reality TV</td>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  category\n",
              "0  Sharon Accepts Plan to Reduce Gaza Army Operat...     World\n",
              "1  Internet Key Battleground in Wildlife Crime Fight  Sci/Tech\n",
              "2          July Durable Good Orders Rise 1.7 Percent  Business\n",
              "3          Growing Signs of a Slowing on Wall Street  Business\n",
              "4                        The New Faces of Reality TV     World"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR_lLiVBDxW1"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u00AZ0O8DxaY"
      },
      "source": [
        "We're going to clean up our input data first by doing operations such as lower text, removing stop (filler) words, filters using regular expressions, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq6a11YsDvEU"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import re"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5IpVaKqDvG1",
        "outputId": "2f59b82e-d839-441c-86e7-4243c6db6ba4"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "STOPWORDS = stopwords.words('english')\n",
        "print (STOPWORDS[:5])\n",
        "porter = PorterStemmer()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "['i', 'me', 'my', 'myself', 'we']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1n4w3KLQDvJk"
      },
      "source": [
        "def preprocess(text, stopwords=STOPWORDS):\n",
        "    \"\"\"Conditional preprocessing on our text unique to our task.\"\"\"\n",
        "    # Lower\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove stopwords\n",
        "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords) + r')\\b\\s*')\n",
        "    text = pattern.sub('', text)\n",
        "\n",
        "    # Remove words in paranthesis\n",
        "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
        "\n",
        "    # Spacing and filters\n",
        "    text = re.sub(r\"([-;;.,!?<=>])\", r\" \\1 \", text)\n",
        "    text = re.sub('[^A-Za-z0-9]+', ' ', text) # remove non alphanumeric chars\n",
        "    text = re.sub(' +', ' ', text)  # remove multiple spaces\n",
        "    text = text.strip()\n",
        "\n",
        "    return text"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "U9JmpiCmDvMz",
        "outputId": "1edc59cb-7f4e-43ea-c008-ded886588ac0"
      },
      "source": [
        "# Sample\n",
        "text = \"Great week for the NYSE!\"\n",
        "preprocess(text=text)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'great week nyse'"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RZ45-N9DvQD",
        "outputId": "4f03b82f-bf7e-480b-9dfd-1a27c1bbc0b3"
      },
      "source": [
        "# Apply to dataframe\n",
        "preprocessed_df = df.copy()\n",
        "preprocessed_df.title = preprocessed_df.title.apply(preprocess)\n",
        "print (f\"{df.title.values[0]}\\n\\n{preprocessed_df.title.values[0]}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sharon Accepts Plan to Reduce Gaza Army Operation, Haaretz Says\n",
            "\n",
            "sharon accepts plan reduce gaza army operation haaretz says\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxo6RKCQ71dl"
      },
      "source": [
        "## Split data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS6kCcfY6IHE"
      },
      "source": [
        "import collections\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qkWnO3uYPnk"
      },
      "source": [
        "TRAIN_SIZE = 0.7\n",
        "VAL_SIZE = 0.15\n",
        "TEST_SIZE = 0.15"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gbh2TLBXVDu"
      },
      "source": [
        "def train_val_test_split(X, y, train_size):\n",
        "    \"\"\"Split dataset into data splits.\"\"\"\n",
        "    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size=0.5, stratify=y_)\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqiQd2j_76gP"
      },
      "source": [
        "# Data\n",
        "X = preprocessed_df[\"title\"].values\n",
        "y = preprocessed_df[\"category\"].values"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwdPM136EBMA",
        "outputId": "9abe0d25-ed9d-49cb-a6d3-629b34be0ea4"
      },
      "source": [
        "# Create data splits\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\n",
        "    X=X, y=y, train_size=TRAIN_SIZE)\n",
        "print (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "print (f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
        "print (f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
        "print (f\"Sample point: {X_train[0]} â†’ {y_train[0]}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train: (84000,), y_train: (84000,)\n",
            "X_val: (18000,), y_val: (18000,)\n",
            "X_test: (18000,), y_test: (18000,)\n",
            "Sample point: china battles north korea nuclear talks â†’ World\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUAn5IOjEF8P"
      },
      "source": [
        "## LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNTjndI0EKzm"
      },
      "source": [
        "Next we'll define a `LabelEncoder` to encode our text labels into unique indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgLNA5yTELB1"
      },
      "source": [
        "import itertools"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeoPR77QELJT"
      },
      "source": [
        "class LabelEncoder(object):\n",
        "    \"\"\"Label encoder for tag labels.\"\"\"\n",
        "    def __init__(self, class_to_index={}):\n",
        "        self.class_to_index = class_to_index\n",
        "        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n",
        "        self.classes = list(self.class_to_index.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.class_to_index)\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"<LabelEncoder(num_classes={len(self)})>\"\n",
        "\n",
        "    def fit(self, y):\n",
        "        classes = np.unique(y)\n",
        "        for i, class_ in enumerate(classes):\n",
        "            self.class_to_index[class_] = i\n",
        "        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n",
        "        self.classes = list(self.class_to_index.keys())\n",
        "        return self\n",
        "\n",
        "    def encode(self, y):\n",
        "        encoded = np.zeros((len(y)), dtype=int)\n",
        "        for i, item in enumerate(y):\n",
        "            encoded[i] = self.class_to_index[item]\n",
        "        return encoded\n",
        "\n",
        "    def decode(self, y):\n",
        "        classes = []\n",
        "        for i, item in enumerate(y):\n",
        "            classes.append(self.index_to_class[item])\n",
        "        return classes\n",
        "\n",
        "    def save(self, fp):\n",
        "        with open(fp, 'w') as fp:\n",
        "            contents = {'class_to_index': self.class_to_index}\n",
        "            json.dump(contents, fp, indent=4, sort_keys=False)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, fp):\n",
        "        with open(fp, 'r') as fp:\n",
        "            kwargs = json.load(fp=fp)\n",
        "        return cls(**kwargs)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MtTxMB5ELL4",
        "outputId": "bd92474e-c8d9-4f85-9449-13fd912f5e37"
      },
      "source": [
        "# Encode\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(y_train)\n",
        "NUM_CLASSES = len(label_encoder)\n",
        "label_encoder.class_to_index"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Business': 0, 'Sci/Tech': 1, 'Sports': 2, 'World': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IzPFnoDELSr",
        "outputId": "a3ee0554-b8c4-455d-8084-9b6fddd6f616"
      },
      "source": [
        "# Convert labels to tokens\n",
        "print (f\"y_train[0]: {y_train[0]}\")\n",
        "y_train = label_encoder.encode(y_train)\n",
        "y_val = label_encoder.encode(y_val)\n",
        "y_test = label_encoder.encode(y_test)\n",
        "print (f\"y_train[0]: {y_train[0]}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_train[0]: World\n",
            "y_train[0]: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjESU2TOEQz_",
        "outputId": "1c87bf2d-03f3-40c1-8d1c-06392c9f5019"
      },
      "source": [
        "# Class weights\n",
        "counts = np.bincount(y_train)\n",
        "class_weights = {i: 1.0/count for i, count in enumerate(counts)}\n",
        "print (f\"counts: {counts}\\nweights: {class_weights}\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "counts: [21000 21000 21000 21000]\n",
            "weights: {0: 4.761904761904762e-05, 1: 4.761904761904762e-05, 2: 4.761904761904762e-05, 3: 4.761904761904762e-05}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIfmW7vJ8Jx1"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYREXERdETdQ"
      },
      "source": [
        "We'll define a `Tokenizer` to convert our text input data into token indices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V6v1yeVGxNg"
      },
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "from more_itertools import take"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChhTN8WDY4Z5"
      },
      "source": [
        "class Tokenizer(object):\n",
        "    def __init__(self, char_level, num_tokens=None, \n",
        "                 pad_token='<PAD>', oov_token='<UNK>',\n",
        "                 token_to_index=None):\n",
        "        self.char_level = char_level\n",
        "        self.separator = '' if self.char_level else ' '\n",
        "        if num_tokens: num_tokens -= 2 # pad + unk tokens\n",
        "        self.num_tokens = num_tokens\n",
        "        self.pad_token = pad_token\n",
        "        self.oov_token = oov_token\n",
        "        if not token_to_index:\n",
        "            token_to_index = {pad_token: 0, oov_token: 1}\n",
        "        self.token_to_index = token_to_index\n",
        "        self.index_to_token = {v: k for k, v in self.token_to_index.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_to_index)\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"<Tokenizer(num_tokens={len(self)})>\"\n",
        "\n",
        "    def fit_on_texts(self, texts):\n",
        "        if not self.char_level:\n",
        "            texts = [text.split(\" \") for text in texts]\n",
        "        all_tokens = [token for text in texts for token in text]\n",
        "        counts = Counter(all_tokens).most_common(self.num_tokens)\n",
        "        self.min_token_freq = counts[-1][1]\n",
        "        for token, count in counts:\n",
        "            index = len(self)\n",
        "            self.token_to_index[token] = index\n",
        "            self.index_to_token[index] = token\n",
        "        return self\n",
        "\n",
        "    def texts_to_sequences(self, texts):\n",
        "        sequences = []\n",
        "        for text in texts:\n",
        "            if not self.char_level:\n",
        "                text = text.split(' ')\n",
        "            sequence = []\n",
        "            for token in text:\n",
        "                sequence.append(self.token_to_index.get(\n",
        "                    token, self.token_to_index[self.oov_token]))\n",
        "            sequences.append(np.asarray(sequence))\n",
        "        return sequences\n",
        "\n",
        "    def sequences_to_texts(self, sequences):\n",
        "        texts = []\n",
        "        for sequence in sequences:\n",
        "            text = []\n",
        "            for index in sequence:\n",
        "                text.append(self.index_to_token.get(index, self.oov_token))\n",
        "            texts.append(self.separator.join([token for token in text]))\n",
        "        return texts\n",
        "\n",
        "    def save(self, fp):\n",
        "        with open(fp, 'w') as fp:\n",
        "            contents = {\n",
        "                'char_level': self.char_level,\n",
        "                'oov_token': self.oov_token,\n",
        "                'token_to_index': self.token_to_index\n",
        "            }\n",
        "            json.dump(contents, fp, indent=4, sort_keys=False)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, fp):\n",
        "        with open(fp, 'r') as fp:\n",
        "            kwargs = json.load(fp=fp)\n",
        "        return cls(**kwargs)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylpHSbcZYsXa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f3c6bef-a08a-42c1-b14f-6af7995cc963"
      },
      "source": [
        "# Tokenize\n",
        "tokenizer = Tokenizer(char_level=False, num_tokens=5000)\n",
        "tokenizer.fit_on_texts(texts=X_train)\n",
        "VOCAB_SIZE = len(tokenizer)\n",
        "print (tokenizer)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Tokenizer(num_tokens=5000)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVgBSnFTfE5D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f97eb536-3e81-42cd-f1b6-0167cb62f809"
      },
      "source": [
        "# Sample of tokens\n",
        "print (take(5, tokenizer.token_to_index.items()))\n",
        "print (f\"least freq token's freq: {tokenizer.min_token_freq}\") # use this to adjust num_tokens"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('<PAD>', 0), ('<UNK>', 1), ('39', 2), ('b', 3), ('gt', 4)]\n",
            "least freq token's freq: 14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcscM_vL8KvP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2eeb74d-d290-4306-fc4d-a1a5bc5e6716"
      },
      "source": [
        "# Convert texts to sequences of indices\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "preprocessed_text = tokenizer.sequences_to_texts([X_train[0]])[0]\n",
        "print (\"Text to indices:\\n\"\n",
        "    f\"  (preprocessed) â†’ {preprocessed_text}\\n\"\n",
        "    f\"  (tokenized) â†’ {X_train[0]}\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text to indices:\n",
            "  (preprocessed) â†’ china battles north korea nuclear talks\n",
            "  (tokenized) â†’ [  16 1491  285  142  114   24]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9o91b6kKHjN4"
      },
      "source": [
        "## Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD6CuPxOHlsw"
      },
      "source": [
        "We'll need to do 2D padding to our tokenized text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcNEooUHHkwn"
      },
      "source": [
        "def pad_sequences(sequences, max_seq_len=0):\n",
        "    \"\"\"Pad sequences to max length in sequence.\"\"\"\n",
        "    max_seq_len = max(max_seq_len, max(len(sequence) for sequence in sequences))\n",
        "    padded_sequences = np.zeros((len(sequences), max_seq_len))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        padded_sequences[i][:len(sequence)] = sequence\n",
        "    return padded_sequences"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfWHamHZHtT-",
        "outputId": "1f079464-4a50-4916-bb41-e65affd2a944"
      },
      "source": [
        "# 2D sequences\n",
        "padded = pad_sequences(X_train[0:3])\n",
        "print (padded.shape)\n",
        "print (padded)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 6)\n",
            "[[1.600e+01 1.491e+03 2.850e+02 1.420e+02 1.140e+02 2.400e+01]\n",
            " [1.445e+03 2.300e+01 6.560e+02 2.197e+03 1.000e+00 0.000e+00]\n",
            " [1.200e+02 1.400e+01 1.955e+03 1.005e+03 1.529e+03 4.014e+03]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoWQk0hO9bK2"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSPbGwOkHvfj"
      },
      "source": [
        "We're going to create Datasets and DataLoaders to be able to efficiently create batches with our data splits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSdfc0-cb9fc"
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"<Dataset(N={len(self)})>\"\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        X = self.X[index]\n",
        "        y = self.y[index]\n",
        "        return [X, len(X), y]\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        \"\"\"Processing on a batch.\"\"\"\n",
        "        # Get inputs\n",
        "        batch = np.array(batch, dtype=object)\n",
        "        X = batch[:, 0]\n",
        "        seq_lens = batch[:, 1]\n",
        "        y = np.stack(batch[:, 2], axis=0)\n",
        "\n",
        "        # Pad inputs\n",
        "        X = pad_sequences(sequences=X)\n",
        "\n",
        "        # Cast\n",
        "        X = torch.LongTensor(X.astype(np.int32))\n",
        "        seq_lens = torch.LongTensor(seq_lens.astype(np.int32))\n",
        "        y = torch.LongTensor(y.astype(np.int32))\n",
        "\n",
        "        return X, seq_lens, y\n",
        "\n",
        "    def create_dataloader(self, batch_size, shuffle=False, drop_last=False):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            dataset=self, batch_size=batch_size, collate_fn=self.collate_fn,\n",
        "            shuffle=shuffle, drop_last=drop_last, pin_memory=True)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYZtzbWMIXY7",
        "outputId": "b85875dc-b6a8-41c9-a327-aa42662ac34b"
      },
      "source": [
        "# Create datasets\n",
        "train_dataset = Dataset(X=X_train, y=y_train)\n",
        "val_dataset = Dataset(X=X_val, y=y_val)\n",
        "test_dataset = Dataset(X=X_test, y=y_test)\n",
        "print (\"Datasets:\\n\"\n",
        "    f\"  Train dataset:{train_dataset.__str__()}\\n\"\n",
        "    f\"  Val dataset: {val_dataset.__str__()}\\n\"\n",
        "    f\"  Test dataset: {test_dataset.__str__()}\\n\"\n",
        "    \"Sample point:\\n\"\n",
        "    f\"  X: {train_dataset[0][0]}\\n\"\n",
        "    f\"  seq_len: {train_dataset[0][1]}\\n\"\n",
        "    f\"  y: {train_dataset[0][2]}\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Datasets:\n",
            "  Train dataset:<Dataset(N=84000)>\n",
            "  Val dataset: <Dataset(N=18000)>\n",
            "  Test dataset: <Dataset(N=18000)>\n",
            "Sample point:\n",
            "  X: [  16 1491  285  142  114   24]\n",
            "  seq_len: 6\n",
            "  y: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w6wVKJe9fxk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b79b3ec-6a35-4afc-e071-863bd57e7996"
      },
      "source": [
        "# Create dataloaders\n",
        "batch_size = 64\n",
        "train_dataloader = train_dataset.create_dataloader(\n",
        "    batch_size=batch_size)\n",
        "val_dataloader = val_dataset.create_dataloader(\n",
        "    batch_size=batch_size)\n",
        "test_dataloader = test_dataset.create_dataloader(\n",
        "    batch_size=batch_size)\n",
        "batch_X, batch_seq_lens, batch_y = next(iter(train_dataloader))\n",
        "print (\"Sample batch:\\n\"\n",
        "    f\"  X: {list(batch_X.size())}\\n\"\n",
        "    f\"  seq_lens: {list(batch_seq_lens.size())}\\n\"\n",
        "    f\"  y: {list(batch_y.size())}\\n\"\n",
        "    \"Sample point:\\n\"\n",
        "    f\"  X: {batch_X[0]}\\n\"\n",
        "    f\" seq_len: {batch_seq_lens[0]}\\n\"\n",
        "    f\"  y: {batch_y[0]}\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample batch:\n",
            "  X: [64, 14]\n",
            "  seq_lens: [64]\n",
            "  y: [64]\n",
            "Sample point:\n",
            "  X: tensor([  16, 1491,  285,  142,  114,   24,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0], device='cpu')\n",
            " seq_len: 6\n",
            "  y: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB-g9F1TJOPx"
      },
      "source": [
        "## Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL5FcX0hJR37"
      },
      "source": [
        "Let's create the `Trainer` class that we'll use to facilitate training for our experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5KWF-DXJN2a"
      },
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self, model, device, loss_fn=None, optimizer=None, scheduler=None):\n",
        "\n",
        "        # Set params\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "    def train_step(self, dataloader):\n",
        "        \"\"\"Train step.\"\"\"\n",
        "        # Set model to train mode\n",
        "        self.model.train()\n",
        "        loss = 0.0\n",
        "\n",
        "        # Iterate over train batches\n",
        "        for i, batch in enumerate(dataloader):\n",
        "\n",
        "            # Step\n",
        "            batch = [item.to(self.device) for item in batch]  # Set device\n",
        "            inputs, targets = batch[:-1], batch[-1]\n",
        "            self.optimizer.zero_grad()  # Reset gradients\n",
        "            z = self.model(inputs)  # Forward pass\n",
        "            J = self.loss_fn(z, targets)  # Define loss\n",
        "            J.backward()  # Backward pass\n",
        "            self.optimizer.step()  # Update weights\n",
        "\n",
        "            # Cumulative Metrics\n",
        "            loss += (J.detach().item() - loss) / (i + 1)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def eval_step(self, dataloader):\n",
        "        \"\"\"Validation or test step.\"\"\"\n",
        "        # Set model to eval mode\n",
        "        self.model.eval()\n",
        "        loss = 0.0\n",
        "        y_trues, y_probs = [], []\n",
        "\n",
        "        # Iterate over val batches\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(dataloader):\n",
        "\n",
        "                # Step\n",
        "                batch = [item.to(self.device) for item in batch]  # Set device\n",
        "                inputs, y_true = batch[:-1], batch[-1]\n",
        "                z = self.model(inputs)  # Forward pass\n",
        "                J = self.loss_fn(z, y_true).item()\n",
        "\n",
        "                # Cumulative Metrics\n",
        "                loss += (J - loss) / (i + 1)\n",
        "\n",
        "                # Store outputs\n",
        "                y_prob = torch.sigmoid(z).cpu().numpy()\n",
        "                y_probs.extend(y_prob)\n",
        "                y_trues.extend(y_true.cpu().numpy())\n",
        "\n",
        "        return loss, np.vstack(y_trues), np.vstack(y_probs)\n",
        "\n",
        "    def predict_step(self, dataloader):\n",
        "        \"\"\"Prediction step.\"\"\"\n",
        "        # Set model to eval mode\n",
        "        self.model.eval()\n",
        "        y_probs = []\n",
        "\n",
        "        # Iterate over val batches\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(dataloader):\n",
        "\n",
        "                # Forward pass w/ inputs\n",
        "                inputs, targets = batch[:-1], batch[-1]\n",
        "                y_prob = self.model(inputs, apply_softmax=True)\n",
        "\n",
        "                # Store outputs\n",
        "                y_probs.extend(y_prob)\n",
        "\n",
        "        return np.vstack(y_probs)\n",
        "    \n",
        "    def train(self, num_epochs, patience, train_dataloader, val_dataloader):\n",
        "        best_val_loss = np.inf\n",
        "        for epoch in range(num_epochs):\n",
        "            # Steps\n",
        "            train_loss = self.train_step(dataloader=train_dataloader)\n",
        "            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)\n",
        "            self.scheduler.step(val_loss)\n",
        "\n",
        "            # Early stopping\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_model = self.model\n",
        "                _patience = patience  # reset _patience\n",
        "            else:\n",
        "                _patience -= 1\n",
        "            if not _patience:  # 0\n",
        "                print(\"Stopping early!\")\n",
        "                break\n",
        "\n",
        "            # Logging\n",
        "            print(\n",
        "                f\"Epoch: {epoch+1} | \"\n",
        "                f\"train_loss: {train_loss:.5f}, \"\n",
        "                f\"val_loss: {val_loss:.5f}, \"\n",
        "                f\"lr: {self.optimizer.param_groups[0]['lr']:.2E}, \"\n",
        "                f\"_patience: {_patience}\"\n",
        "            )\n",
        "        return best_model"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUMibnFtuq_i"
      },
      "source": [
        "# Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO-KQgMF1jEs"
      },
      "source": [
        "Attention applied to the outputs from an RNN. In theory, the outputs can come from anywhere where we want to learn how to weight amongst them but since we're working with the context of an RNN from the previous lesson , we'll continue with that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSPxJXxZu4Jo"
      },
      "source": [
        "<div align=\"left\">\n",
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/images/foundations/attention/attention.png\" width=\"500\">\n",
        "</div>\n",
        "\n",
        "$ \\alpha = softmax(W_{attn}h) $\n",
        "\n",
        "$c_t = \\sum_{i=1}^{n} \\alpha_{t,i}h_i $\n",
        "\n",
        "\n",
        "*where*:\n",
        "* $ h $ = RNN outputs (or any group of outputs you want to attend to) $\\in \\mathbb{R}^{NXMXH}$ ($N$ is the batch size, $M$ is the max length of each sequence in the batch, $H$ is the hidden dim)\n",
        "* $ \\alpha_{t,i} $ = alignment function for output $ y_t $ using input $ h_i $ (we also concatenate other useful representations with $h_i$ here). In our case, this would be the attention value to attribute to each input $h_i$. \n",
        "* $W_{attn}$ = attention weights to learn $\\in \\mathbb{R}^{HX1}$. We can also apply activations functions, transformations, etc. here\n",
        "* $c_t$ = context vector that accounts for the different inputs with attention. We can pass this context vector to downstream processes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeGSvWrl73Nz"
      },
      "source": [
        "import torch.nn.functional as F"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUf0SD2N3IUy"
      },
      "source": [
        "The RNN will create an encoded representation for each word in our input resulting in a stacked vector that has dimensions $NXMXH$, where N is the # of samples in the batch, M is the max sequence length in the batch, and H is the number of hidden units in the RNN. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZovWtFy3FuC"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "SEQ_LEN = 8\n",
        "EMBEDDING_DIM = 100\n",
        "RNN_HIDDEN_DIM = 128"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFK6L0nj5H9s"
      },
      "source": [
        "# Embed\n",
        "x = torch.rand((BATCH_SIZE, SEQ_LEN, EMBEDDING_DIM))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWoF4mU-3FxC",
        "outputId": "fcf27520-e380-4c6f-9a70-adf742d85743"
      },
      "source": [
        "# Encode\n",
        "rnn = nn.RNN(EMBEDDING_DIM, RNN_HIDDEN_DIM, batch_first=True)\n",
        "out, h_n = rnn(x) # h_n is the last hidden state\n",
        "print (\"out: \", out.shape)\n",
        "print (\"h_n: \", h_n.shape)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "out:  torch.Size([64, 8, 128])\n",
            "h_n:  torch.Size([1, 64, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYMes2oj3Fzn",
        "outputId": "cab8a9cb-7a95-4583-eece-5da74e8d110c"
      },
      "source": [
        "# Attend\n",
        "attn = nn.Linear(RNN_HIDDEN_DIM, 1)\n",
        "e = attn(out)\n",
        "attn_vals = F.softmax(e.squeeze(2), dim=1)\n",
        "c = torch.bmm(attn_vals.unsqueeze(1), out).squeeze(1)\n",
        "print (\"e: \", e.shape)\n",
        "print (\"attn_vals: \", attn_vals.shape)\n",
        "print (\"attn_vals[0]: \", attn_vals[0])\n",
        "print (\"sum(attn_vals[0]): \", sum(attn_vals[0]))\n",
        "print (\"c: \", c.shape)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "e:  torch.Size([64, 8, 1])\n",
            "attn_vals:  torch.Size([64, 8])\n",
            "attn_vals[0]:  tensor([0.1240, 0.1109, 0.1269, 0.1284, 0.1398, 0.0994, 0.1218, 0.1488],\n",
            "       grad_fn=<SelectBackward>)\n",
            "sum(attn_vals[0]):  tensor(1., grad_fn=<AddBackward0>)\n",
            "c:  torch.Size([64, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdAbPbqM3F20",
        "outputId": "da0d39c2-ce21-47b2-ddf0-f7de56945946"
      },
      "source": [
        "# Predict\n",
        "fc1 = nn.Linear(RNN_HIDDEN_DIM, NUM_CLASSES)\n",
        "output = F.softmax(fc1(c), dim=1)\n",
        "print (\"output: \", output.shape)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output:  torch.Size([64, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U_s9SSY3xmt"
      },
      "source": [
        "> In a many-to-many task such as machine translation, our attentional interface will also account for the encoded representation of token in the output as well (via concatenation) so we can know which encoded inputs to attend to based on the encoded output we're focusing on. For more on this, be sure to explore <a target=\"_blank\" href=\"https://arxiv.org/abs/1409.0473\">Bahdanau's attention paper</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfhjWZRD94hK"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1mtPyJ725-t"
      },
      "source": [
        "Now let's create our RNN based model but with the addition of the attention layer on top of the RNN's outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoBC0u8XpbVv"
      },
      "source": [
        "RNN_HIDDEN_DIM = 128\n",
        "DROPOUT_P = 0.1\n",
        "HIDDEN_DIM = 100"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPP5ROd69mXC"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, embedding_dim, vocab_size, rnn_hidden_dim,\n",
        "                 hidden_dim, dropout_p, num_classes, padding_idx=0):\n",
        "        super(RNN, self).__init__()\n",
        "        \n",
        "        # Initialize embeddings\n",
        "        self.embeddings = nn.Embedding(\n",
        "            embedding_dim=embedding_dim, num_embeddings=vocab_size,\n",
        "            padding_idx=padding_idx)\n",
        "        \n",
        "        # RNN\n",
        "        self.rnn = nn.RNN(embedding_dim, rnn_hidden_dim, batch_first=True)\n",
        "\n",
        "        # Attention\n",
        "        self.attn = nn.Linear(rnn_hidden_dim, 1)\n",
        "     \n",
        "        # FC weights\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.fc1 = nn.Linear(rnn_hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, inputs, apply_softmax=False):\n",
        "        # Embed\n",
        "        x_in, seq_lens = inputs\n",
        "        x_in = self.embeddings(x_in)\n",
        "            \n",
        "        # Encode\n",
        "        out, h_n = self.rnn(x_in)\n",
        "        \n",
        "        # Attend\n",
        "        e = self.attn(out)\n",
        "        attn_vals = F.softmax(e.squeeze(2), dim=1)\n",
        "        c = torch.bmm(attn_vals.unsqueeze(1), out).squeeze(1)\n",
        "\n",
        "        # Predict\n",
        "        z = self.fc1(c)\n",
        "        z = self.dropout(z)\n",
        "        y_pred = self.fc2(z)\n",
        "        \n",
        "        if apply_softmax:\n",
        "            y_pred = F.softmax(y_pred, dim=1)\n",
        "        return y_pred"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LH_z2FqUn5DR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4153e4b7-8bfc-4906-a72e-719dde0bdce1"
      },
      "source": [
        "# Simple RNN cell\n",
        "model = RNN(\n",
        "    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE, \n",
        "    rnn_hidden_dim=RNN_HIDDEN_DIM, hidden_dim=HIDDEN_DIM, \n",
        "    dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)\n",
        "model = model.to(device) # set device\n",
        "print (model.named_parameters)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Module.named_parameters of RNN(\n",
            "  (embeddings): Embedding(5000, 100, padding_idx=0)\n",
            "  (rnn): RNN(100, 128, batch_first=True)\n",
            "  (attn): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (fc1): Linear(in_features=128, out_features=100, bias=True)\n",
            "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9ggYO6yHIm2"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geKOPVzVK6S9"
      },
      "source": [
        "from torch.optim import Adam"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfSBvenCiIce"
      },
      "source": [
        "NUM_LAYERS = 1\n",
        "LEARNING_RATE = 1e-4\n",
        "PATIENCE = 10\n",
        "NUM_EPOCHS = 50"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ilmlIMUzNIj"
      },
      "source": [
        "# Define Loss\n",
        "class_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ucn3tYq1_sE1"
      },
      "source": [
        "# Define optimizer & scheduler\n",
        "optimizer = Adam(model.parameters(), lr=LEARNING_RATE) \n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.1, patience=3)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5ac-uJXb-F7"
      },
      "source": [
        "# Trainer module\n",
        "trainer = Trainer(\n",
        "    model=model, device=device, loss_fn=loss_fn, \n",
        "    optimizer=optimizer, scheduler=scheduler)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-uKcZrABKVZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cad1f9ab-330b-44b1-83e4-63f739b0920a"
      },
      "source": [
        "# Train\n",
        "best_model = trainer.train(\n",
        "    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 | train_loss: 1.21680, val_loss: 1.08622, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 2 | train_loss: 1.00379, val_loss: 0.93546, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 3 | train_loss: 0.87091, val_loss: 0.83399, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 4 | train_loss: 0.77709, val_loss: 0.76286, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 5 | train_loss: 0.70787, val_loss: 0.71245, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 6 | train_loss: 0.65572, val_loss: 0.67600, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 7 | train_loss: 0.61451, val_loss: 0.64830, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 8 | train_loss: 0.58117, val_loss: 0.62701, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 9 | train_loss: 0.55371, val_loss: 0.60928, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 10 | train_loss: 0.53038, val_loss: 0.59558, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 11 | train_loss: 0.51044, val_loss: 0.58470, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 12 | train_loss: 0.49248, val_loss: 0.57755, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 13 | train_loss: 0.47619, val_loss: 0.56935, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 14 | train_loss: 0.46190, val_loss: 0.56498, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 15 | train_loss: 0.44878, val_loss: 0.56161, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 16 | train_loss: 0.43703, val_loss: 0.56042, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 17 | train_loss: 0.42491, val_loss: 0.55836, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 18 | train_loss: 0.41489, val_loss: 0.56118, lr: 1.00E-04, _patience: 9\n",
            "Epoch: 19 | train_loss: 0.40539, val_loss: 0.56346, lr: 1.00E-04, _patience: 8\n",
            "Epoch: 20 | train_loss: 0.39620, val_loss: 0.56318, lr: 1.00E-04, _patience: 7\n",
            "Epoch: 21 | train_loss: 0.38681, val_loss: 0.56822, lr: 1.00E-05, _patience: 6\n",
            "Epoch: 22 | train_loss: 0.36529, val_loss: 0.54859, lr: 1.00E-05, _patience: 10\n",
            "Epoch: 23 | train_loss: 0.36016, val_loss: 0.54929, lr: 1.00E-05, _patience: 9\n",
            "Epoch: 24 | train_loss: 0.35843, val_loss: 0.55053, lr: 1.00E-05, _patience: 8\n",
            "Epoch: 25 | train_loss: 0.35740, val_loss: 0.55103, lr: 1.00E-05, _patience: 7\n",
            "Epoch: 26 | train_loss: 0.35573, val_loss: 0.55192, lr: 1.00E-06, _patience: 6\n",
            "Epoch: 27 | train_loss: 0.35250, val_loss: 0.54854, lr: 1.00E-06, _patience: 10\n",
            "Epoch: 28 | train_loss: 0.35196, val_loss: 0.54809, lr: 1.00E-06, _patience: 10\n",
            "Epoch: 29 | train_loss: 0.35197, val_loss: 0.54803, lr: 1.00E-06, _patience: 10\n",
            "Epoch: 30 | train_loss: 0.35144, val_loss: 0.54800, lr: 1.00E-06, _patience: 10\n",
            "Epoch: 31 | train_loss: 0.35184, val_loss: 0.54805, lr: 1.00E-06, _patience: 9\n",
            "Epoch: 32 | train_loss: 0.35131, val_loss: 0.54805, lr: 1.00E-06, _patience: 8\n",
            "Epoch: 33 | train_loss: 0.35130, val_loss: 0.54806, lr: 1.00E-07, _patience: 7\n",
            "Epoch: 34 | train_loss: 0.35092, val_loss: 0.54739, lr: 1.00E-07, _patience: 10\n",
            "Epoch: 35 | train_loss: 0.35087, val_loss: 0.54727, lr: 1.00E-07, _patience: 10\n",
            "Epoch: 36 | train_loss: 0.35053, val_loss: 0.54725, lr: 1.00E-07, _patience: 10\n",
            "Epoch: 37 | train_loss: 0.35081, val_loss: 0.54722, lr: 1.00E-07, _patience: 10\n",
            "Epoch: 38 | train_loss: 0.35062, val_loss: 0.54720, lr: 1.00E-07, _patience: 10\n",
            "Epoch: 39 | train_loss: 0.35072, val_loss: 0.54723, lr: 1.00E-07, _patience: 9\n",
            "Epoch: 40 | train_loss: 0.35099, val_loss: 0.54722, lr: 1.00E-07, _patience: 8\n",
            "Epoch: 41 | train_loss: 0.35089, val_loss: 0.54723, lr: 1.00E-07, _patience: 7\n",
            "Epoch: 42 | train_loss: 0.35068, val_loss: 0.54722, lr: 1.00E-08, _patience: 6\n",
            "Epoch: 43 | train_loss: 0.35057, val_loss: 0.54721, lr: 1.00E-08, _patience: 5\n",
            "Epoch: 44 | train_loss: 0.35043, val_loss: 0.54721, lr: 1.00E-08, _patience: 4\n",
            "Epoch: 45 | train_loss: 0.35048, val_loss: 0.54720, lr: 1.00E-08, _patience: 10\n",
            "Epoch: 46 | train_loss: 0.35052, val_loss: 0.54719, lr: 1.00E-08, _patience: 10\n",
            "Epoch: 47 | train_loss: 0.35075, val_loss: 0.54719, lr: 1.00E-08, _patience: 10\n",
            "Epoch: 48 | train_loss: 0.35045, val_loss: 0.54718, lr: 1.00E-08, _patience: 10\n",
            "Epoch: 49 | train_loss: 0.35055, val_loss: 0.54718, lr: 1.00E-08, _patience: 10\n",
            "Epoch: 50 | train_loss: 0.35086, val_loss: 0.54717, lr: 1.00E-08, _patience: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axytMjGQmz6T"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ly3zjVAm4Wr"
      },
      "source": [
        "import json\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojOfz9rjm4ZI"
      },
      "source": [
        "def get_performance(y_true, y_pred, classes):\n",
        "    \"\"\"Per-class performance metrics.\"\"\"\n",
        "    # Performance\n",
        "    performance = {\"overall\": {}, \"class\": {}}\n",
        "\n",
        "    # Overall performance\n",
        "    metrics = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
        "    performance[\"overall\"][\"precision\"] = metrics[0]\n",
        "    performance[\"overall\"][\"recall\"] = metrics[1]\n",
        "    performance[\"overall\"][\"f1\"] = metrics[2]\n",
        "    performance[\"overall\"][\"num_samples\"] = np.float64(len(y_true))\n",
        "\n",
        "    # Per-class performance\n",
        "    metrics = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
        "    for i in range(len(classes)):\n",
        "        performance[\"class\"][classes[i]] = {\n",
        "            \"precision\": metrics[0][i],\n",
        "            \"recall\": metrics[1][i],\n",
        "            \"f1\": metrics[2][i],\n",
        "            \"num_samples\": np.float64(metrics[3][i]),\n",
        "        }\n",
        "\n",
        "    return performance"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tho3e_yQzqZ1"
      },
      "source": [
        "# Get predictions\n",
        "test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)\n",
        "y_pred = np.argmax(y_prob, axis=1)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fylHduxTK-0N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5813ddb2-b772-42d9-94cc-69dffd4c9eed"
      },
      "source": [
        "# Determine performance\n",
        "performance = get_performance(\n",
        "    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\n",
        "print (json.dumps(performance['overall'], indent=2))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"precision\": 0.8133385428975775,\n",
            "  \"recall\": 0.8137222222222222,\n",
            "  \"f1\": 0.8133454847232977,\n",
            "  \"num_samples\": 18000.0\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r9EvThuTLli"
      },
      "source": [
        "# Save artifacts\n",
        "from pathlib import Path\n",
        "dir = Path(\"rnn\")\n",
        "dir.mkdir(parents=True, exist_ok=True)\n",
        "label_encoder.save(fp=Path(dir, \"label_encoder.json\"))\n",
        "tokenizer.save(fp=Path(dir, \"tokenizer.json\"))\n",
        "torch.save(best_model.state_dict(), Path(dir, \"model.pt\"))\n",
        "with open(Path(dir, \"performance.json\"), \"w\") as fp:\n",
        "    json.dump(performance, indent=2, sort_keys=False, fp=fp)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcMSfbYwQKz2"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1CQ5nqNQM9R"
      },
      "source": [
        "def get_probability_distribution(y_prob, classes):\n",
        "    \"\"\"Create a dict of class probabilities from an array.\"\"\"\n",
        "    results = {}\n",
        "    for i, class_ in enumerate(classes):\n",
        "        results[class_] = np.float64(y_prob[i])\n",
        "    sorted_results = {k: v for k, v in sorted(\n",
        "        results.items(), key=lambda item: item[1], reverse=True)}\n",
        "    return sorted_results"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUEYVoHLQNu_",
        "outputId": "07665209-2212-4806-d709-2dc959ca5cff"
      },
      "source": [
        "# Load artifacts\n",
        "device = torch.device(\"cpu\")\n",
        "label_encoder = LabelEncoder.load(fp=Path(dir, \"label_encoder.json\"))\n",
        "tokenizer = Tokenizer.load(fp=Path(dir, \"tokenizer.json\"))\n",
        "model = RNN(\n",
        "    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE, \n",
        "    rnn_hidden_dim=RNN_HIDDEN_DIM, hidden_dim=HIDDEN_DIM, \n",
        "    dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)\n",
        "model.load_state_dict(torch.load(Path(dir, \"model.pt\"), map_location=device))\n",
        "model.to(device)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embeddings): Embedding(5000, 100, padding_idx=0)\n",
              "  (rnn): RNN(100, 128, batch_first=True)\n",
              "  (attn): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc1): Linear(in_features=128, out_features=100, bias=True)\n",
              "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk3Jawc4QNxm"
      },
      "source": [
        "# Initialize trainer\n",
        "trainer = Trainer(model=model, device=device)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_PXGD8hQgYX",
        "outputId": "34648570-6ca6-49f5-c953-7258416255b2"
      },
      "source": [
        "# Dataloader\n",
        "text = \"The final tennis tournament starts next week.\"\n",
        "X = tokenizer.texts_to_sequences([preprocess(text)])\n",
        "print (tokenizer.sequences_to_texts(X))\n",
        "y_filler = label_encoder.encode([label_encoder.classes[0]]*len(X))\n",
        "dataset = Dataset(X=X, y=y_filler)\n",
        "dataloader = dataset.create_dataloader(batch_size=batch_size)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['final tennis tournament starts next week']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWcn1RNuQgaw",
        "outputId": "628daabc-538f-4b46-ddcc-157f845b6c78"
      },
      "source": [
        "# Inference\n",
        "y_prob = trainer.predict_step(dataloader)\n",
        "y_pred = np.argmax(y_prob, axis=1)\n",
        "label_encoder.decode(y_pred)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sports']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdlfiMBeQgdO",
        "outputId": "28fa5aa7-3527-418d-9a45-30a57fceb0f0"
      },
      "source": [
        "# Class distributions\n",
        "prob_dist = get_probability_distribution(y_prob=y_prob[0], classes=label_encoder.classes)\n",
        "print (json.dumps(prob_dist, indent=2))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"Sports\": 0.9651875495910645,\n",
            "  \"World\": 0.03468644618988037,\n",
            "  \"Sci/Tech\": 8.490968320984393e-05,\n",
            "  \"Business\": 4.112234091735445e-05\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fckDDlLrNyqK"
      },
      "source": [
        "# Interpretability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkXEjvrYN0Wi"
      },
      "source": [
        "Let's use the attention values to see which encoded tokens were most useful in predicting the appropriate label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szYYuvRCNxkG"
      },
      "source": [
        "import collections\n",
        "import seaborn as sns"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyapDiMQNx6a"
      },
      "source": [
        "class InterpretAttn(nn.Module):\n",
        "    def __init__(self, embedding_dim, vocab_size, rnn_hidden_dim,\n",
        "                 hidden_dim, dropout_p, num_classes, padding_idx=0):\n",
        "        super(InterpretAttn, self).__init__()\n",
        "        \n",
        "        # Initialize embeddings\n",
        "        self.embeddings = nn.Embedding(\n",
        "            embedding_dim=embedding_dim, num_embeddings=vocab_size,\n",
        "            padding_idx=padding_idx)\n",
        "        \n",
        "        # RNN\n",
        "        self.rnn = nn.RNN(embedding_dim, rnn_hidden_dim, batch_first=True)\n",
        "\n",
        "        # Attention\n",
        "        self.attn = nn.Linear(rnn_hidden_dim, 1)\n",
        "     \n",
        "        # FC weights\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.fc1 = nn.Linear(rnn_hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, inputs, apply_softmax=False):\n",
        "        # Embed\n",
        "        x_in, seq_lens = inputs\n",
        "        x_in = self.embeddings(x_in)\n",
        "            \n",
        "        # Encode\n",
        "        out, h_n = self.rnn(x_in)\n",
        "        \n",
        "        # Attend\n",
        "        e = self.attn(out)  # could add optional activation function (ex. tanh)\n",
        "        attn_vals = F.softmax(e.squeeze(2), dim=1)\n",
        "\n",
        "        return attn_vals"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zt0cUyubNx9M",
        "outputId": "573e75d1-5ff9-4d2b-b9ec-d03a7decf302"
      },
      "source": [
        "# Initialize model\n",
        "interpretable_model = InterpretAttn(\n",
        "    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE, \n",
        "    rnn_hidden_dim=RNN_HIDDEN_DIM, hidden_dim=HIDDEN_DIM, \n",
        "    dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)\n",
        "interpretable_model.load_state_dict(torch.load(Path(dir, \"model.pt\"), map_location=device))\n",
        "interpretable_model.to(device)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "InterpretAttn(\n",
              "  (embeddings): Embedding(5000, 100, padding_idx=0)\n",
              "  (rnn): RNN(100, 128, batch_first=True)\n",
              "  (attn): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc1): Linear(in_features=128, out_features=100, bias=True)\n",
              "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfGP5_Uaqi6q"
      },
      "source": [
        "# Initialize trainer\n",
        "interpretable_trainer = Trainer(model=interpretable_model, device=device)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C11Mz8iqqn3l",
        "outputId": "f6405c4c-5de5-40fa-df93-e4d3c986ab0e"
      },
      "source": [
        "# Get attention values\n",
        "attn_vals  = interpretable_trainer.predict_step(dataloader)\n",
        "print (attn_vals.shape) # (N, max_seq_len)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "2ZOtnF0Jqn6i",
        "outputId": "61185ff7-59da-4d87-cfaf-1cea98818cbe"
      },
      "source": [
        "# Visualize a bi-gram filter's outputs\n",
        "sns.set(rc={\"figure.figsize\":(10, 1)})\n",
        "tokens = tokenizer.sequences_to_texts(X)[0].split(' ')\n",
        "sns.heatmap(attn_vals, xticklabels=tokens)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f157d656850>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x72 with 2 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kI48Yh_rQKH"
      },
      "source": [
        "The word `tennis` was attended to the most to result in the `Sports` label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH1-PtTiXAd2"
      },
      "source": [
        "# Types of attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WQQaq7AZaAt"
      },
      "source": [
        "We'll briefly look at the different types of attention and when to use each them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VryQtkJyZhH7"
      },
      "source": [
        "## Soft (global) attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9FEf9XfZ6Fp"
      },
      "source": [
        "Soft attention the type of attention we've implemented so far, where we attend to all encoded inputs when creating our context vector.\n",
        "\n",
        "- **advantages**: we always have the ability to attend to all inputs in case something we saw much earlier/ see later are crucial for determing the output.\n",
        "- **disadvantages**: if our input sequence is very long, this can lead to expensive compute."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPUxLrMEZkou"
      },
      "source": [
        "## Hard attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyUrS6kqbCUT"
      },
      "source": [
        "Hard attention is focusing on a specific set of the encoded inputs at each time step.\n",
        "\n",
        "- **advantages**: we can save a lot of compute on long sequences by only focusing on a local patch each time.\n",
        "- **disadvantages**: non-differentiable and so we need to use more complex techniques (variance reduction, reinforcement learning, etc.) to train."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcWjZT0mcVWZ"
      },
      "source": [
        "<div align=\"left\">\n",
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/images/foundations/attention/soft_attention.png\" width=\"700\">\n",
        "</div>\n",
        "<div align=\"left\">\n",
        "<small><a href=\"https://arxiv.org/abs/1502.03044\" target=\"_blank\">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a></small>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D56P8LIc_66"
      },
      "source": [
        "## Local attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38RvQIF0dBlg"
      },
      "source": [
        "[Local attention](https://arxiv.org/abs/1508.04025) blends the advantages of soft and hard attention. It involves learning an aligned position vector and empirically determining a local window of encoded inputs to attend to.\n",
        "\n",
        "- **advantages**: apply attention to a local patch of inputs yet remain differentiable.\n",
        "- **disadvantages**: need to determine the alignment vector for each output but it's a worthwhile trade off to determine the right window of inputs to attend to in order to avoid attending to all of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK_oNwI7d3KD"
      },
      "source": [
        "<div align=\"left\">\n",
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/images/foundations/attention/local_attention.png\" width=\"700\">\n",
        "</div>\n",
        "<div align=\"left\">\n",
        "<small><a href=\"https://arxiv.org/abs/1508.04025\" target=\"_blank\">Effective Approaches to Attention-based Neural Machine Translation\n",
        "</a></small>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2asXNHQbZmyu"
      },
      "source": [
        "## Self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_VlFfoQeLM0"
      },
      "source": [
        "We can also use attention within the encoded input sequence to create a weighted representation that based on the similarity between input pairs. This will allow us to create rich representations of the input sequence that are aware of the relationships between each other. For example, in the image below you can see that when composing the representation of the token \"its\", this specific attention head will be incorporating signal from the token \"Law\" (it's learned that \"its\" is referring to the \"Law\").\n",
        "\n",
        "<div align=\"left\">\n",
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/images/foundations/attention/self_attention.png\" width=\"300\">\n",
        "</div>\n",
        "<div align=\"left\">\n",
        "<small><a href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\">Attention Is All You Need</a></small>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5QM3Q2ZeMmw"
      },
      "source": [
        "# Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw7rVfqVAZJB"
      },
      "source": [
        "Transformers are a very popular architecture that leverage and extend the concept of self-attention to create very useful representations of our input data for a downstream task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Bfaa_08Ugmj"
      },
      "source": [
        "## Scaled dot-product attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyAtst8GUrPh"
      },
      "source": [
        "The most popular type of self-attention is scaled dot-product attention from the widely-cited [Attention is all you need](https://arxiv.org/abs/1706.03762) paper. This type of attention involves projecting our encoded input sequences onto three matrices, queries (Q), keys (K) and values (V), whose weights we learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_1jet76X8qV"
      },
      "source": [
        "$ inputs \\in \\mathbb{R}^{NXMXH} $ ($N$ = batch size, $M$ = sequence length, $H$ = hidden dim)\n",
        "\n",
        "$ Q = XW_q $ where $ W_q \\in \\mathbb{R}^{HXd_q} $\n",
        "\n",
        "$ K = XW_k $ where $ W_k \\in \\mathbb{R}^{HXd_k} $\n",
        "\n",
        "$ V = XW_v $ where $ W_v \\in \\mathbb{R}^{HXd_v} $\n",
        "\n",
        "$ attention (Q, K, V) = softmax( \\frac{Q K^{T}}{\\sqrt{d_k}} )V \\in \\mathbb{R}^{MXd_v} $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0xT7ieqT-zY"
      },
      "source": [
        "## Multi-head attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7cwLUkoUBsc"
      },
      "source": [
        "Instead of applying self-attention only once across the entire encoded input, we can also separate the input and apply self-attention in parallel (heads) to each input section and concatenate them. This allows the different head to learn unique representations while maintaining the complexity since we split the input into smaller subspaces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfLNJqhxX6rE"
      },
      "source": [
        "$ MultiHead(Q, K, V) = concat({head}_1, ..., {head}_{h})W_O $ \n",
        "\n",
        "* ${head}_i = attention(Q_i, K_i, V_i) $\n",
        "* $h$ = # of self-attention heads\n",
        "* $W_O \\in \\mathbb{R}^{hd_vXH} $\n",
        "* $H$ = hidden dim. (or dimension of the model $d_{model}$)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCD6cbqgT8Ot"
      },
      "source": [
        "## Positional encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlY6qzamkYoN"
      },
      "source": [
        "With self-attention, we aren't able to account for the sequential position of our input tokens. To address this, we can use positional encoding to create a representation of the location of each token with respect to the entire sequence. This can either be learned (with weights) or we can use a fixed function that can better extend to create positional encoding for lengths during inference that were not observed during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_eN-dKerkt5"
      },
      "source": [
        "$ PE_{(pos,2i)} = sin({pos}/{10000^{2i/H}}) $\n",
        "\n",
        "$ PE_{(pos,2i+1)} = cos({pos}/{10000^{2i/H}}) $\n",
        "\n",
        "where:\n",
        "\n",
        "* $pos$ = position of the token $(1...M)$\n",
        "* $i$ = hidden dim $(1..H)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t70xLrCZrk4X"
      },
      "source": [
        "This effectively allows us to represent each token's relative position using a fixed function for very large sequences. And because we've constrained the positional encodings to have the same dimensions as our encoded inputs, we can simply concatenate them before feeding them into the multi-head attention heads."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkFSjW6QZcFC"
      },
      "source": [
        "## Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67PTrZhhZaj0"
      },
      "source": [
        "And here's how it all fits together! It's an end-to-end architecture that creates these contextual representations and uses an encoder-decoder architecture to predict the outcomes (one-to-one, many-to-one, many-to-many, etc.) Due to the complexity of the architecture, they require massive amounts of data for training without overfitting, however, they can be leveraged as pretrained models to finetune with smaller datasets that are similar to the larger set it was initially trained on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9j-97khZ5Pl"
      },
      "source": [
        "<div align=\"left\">\n",
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/images/foundations/attention/transformer.png\" width=\"800\">\n",
        "</div>\n",
        "<div align=\"left\">\n",
        "<small><a href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\">Attention Is All You Need</a></small>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akYFH3ru_6Tg"
      },
      "source": [
        "> We're not going to the implement the Transformer [from scratch](https://nlp.seas.harvard.edu/2018/04/03/attention.html) but we will use the[ Hugging Face library](https://github.com/huggingface/transformers) to do so in the [baselines](https://madewithml.com/courses/mlops/baselines/#transformers-w-contextual-embeddings) lesson!"
      ]
    }
  ]
}